{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Labor HSKA: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiederholung: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning basiert auf Temporalen Differenz (TD)-Lernen (engl. Temporal Difference Learning). Die Idee von TD Methoden ist es, die bisherige Schätzung der Value-Function zusammen mit dem unmittelbar erhaltenen Reward nach Ausführung einer Aktion zu nutzen, um die bisherige Schätzung der Value-Function zu aktualisieren. Diese Aktualisierung findet nach jeder Aktionswahl statt. Im einfachsten Fall wählt der Agent eine Aktion anhand seiner Policy, erhält eine Belohnung und passt die Value-Function wie folgt an:\n",
    "\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha [\\overbrace{\\underbrace{R_t + \\gamma V(s_{t+1})}_{target} - V(s_t)}^{TD\\ error}]$$\n",
    "\n",
    "Daraus ergibt sich die Lernregel für das Q-Learning:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha\\left[R(s_t,a_t)+\\gamma \\max_{a\\in A(s_{t+1})} Q(s_{t+1},a) - \n",
    "Q(s_t,a_t)\\right]$$\n",
    "\n",
    "$Q$ approximiert dabei $q^*$ und damit die optimale Action-Value Function. Ist $q^*$ bekannt, können optimale Aktionen ohne jegliches Wissen über Nachfolgezustände (Successor States) oder deren Werte gewählt werden, da bereits die zu wählende Aktion für einen Zustand gegeben ist. Das heißt, der agent muss nichts über die Dynamik (Transition Model, Reward Function) der Umgebung wissen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervollständige die in **agent.py** vorgebene Implementierung eines Agent, der mit Hilfe von Q-Learning eine optimale Policy $\\pi^*$ findet. Pro Episode sind maximal 200 Steps in den beiden Ausprägungen der GridWorld erlaubt, danach wird das Environment zurückgesetzt.\n",
    "\n",
    "Der Agent soll über 50 Episoden hinweg versuchen eine optimale Policy zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import GridWorld\n",
    "from lib.statistics import plot\n",
    "\n",
    "n_episodes = 500\n",
    "max_steps = 200\n",
    "\n",
    "def interact_with_environment(env, agent, verbose=False):\n",
    "    statistics = []\n",
    "    if verbose:\n",
    "        print('Startposition:')\n",
    "        env.render()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        #print(\"State 0 in Episode:\",episode, state)\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            #print(\"State\",t,\"in Episode\",episode,\":\", next_state)\n",
    "            agent.train((state, action, next_state, reward, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'episode: {episode}/{n_episodes}, score: {total_reward}, steps:{t}, e:{agent.epsilon:.2f}')\n",
    "            env.render()\n",
    "            \n",
    "        statistics.append({\n",
    "            'episode': episode,\n",
    "            'score': total_reward,\n",
    "            'steps': t\n",
    "        })\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.1\n",
    "\n",
    "Der Agent soll folgende Ausprägung der GridWorld lösen: Die Start- und Zielposition des Agenten ist fest vorgegeben und ändert sich nicht. Das Transition Model dieser GridWorld ist deterministisch (`transition_probability=1.0`). D.h. die vom Agenten gewählte Aktion wird in jedem Fall ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(env_x=8, env_y=8, init_agent_pos=(0,0), goal_pos=(7,7), max_steps=max_steps, transition_probability=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearning\n",
    "\n",
    "# Hyperparams\n",
    "gamma = 0.75\n",
    "epsilon = 0.001\n",
    "epsilon_decay = 0.9\n",
    "epsilon_min = 0.0001\n",
    "alpha = 1.0\n",
    "alpha_decay = 0.999\n",
    "alpha_min = 0.001\n",
    "\n",
    "agent = QLearning(action_dim=env.action_dim, state_dim=env.state_dim,\n",
    "                  gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, verbose=False)\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.2\n",
    "\n",
    "Erweitere (falls nötig) die Implementierung deines Agenten, sodass er die RandomGridWorld (`transition_probability=0.8`) löst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(env_x=8, env_y=8, init_agent_pos=None, goal_pos=None, max_steps=max_steps, transition_probability=0.99)\n",
    "#transition_probability high = less random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearning\n",
    "\n",
    "# Hyperparams\n",
    "gamma = 0.75\n",
    "epsilon = 0.001\n",
    "#epsilon_decay = 0.9\n",
    "#epsilon_min = 0.0001\n",
    "alpha = 0.9\n",
    "#alpha_decay = 0.999\n",
    "#alpha_min = 0.001\n",
    "\n",
    "agent = QLearning(action_dim=env.action_dim, state_dim=env.state_dim,\n",
    "                  gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, verbose=False)\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "Beschreibe was eine optimale Policy ausmacht und begründe warum es mehrere optimale Policies gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Jede Policy, die das GridWorld mit der minimalen Anzahl Schritte durchläuft ist eine optimale Policy. Die minimale Anzahl an Schritten ist (für diese Art von Umgebungen) $N_y + N_x - 2$ (Breite + Höhe - Startzustand - Endzustand)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
