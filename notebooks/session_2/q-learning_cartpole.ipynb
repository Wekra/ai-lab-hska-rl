{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Methode zum Rendern des Zustandes im Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id,step,info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "Löse das CartPole-v0 (https://gym.openai.com/envs/CartPole-v0/) Environment mittels des bereits bekannten Q-Learning. Beachte hierbei, dass es sich um einen continuous state-space handelt und dieser für den tabellelarischen Ansatz zuerst in einen discrete Space transformiert werden muss.\n",
    "\n",
    "**Hinweis:** Für die Lösung müssen nicht alle zur Verfügung stehenden Eingabefeatures genutzt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht über die vom Environment bereitgestellten Features und deren Wertebereiche\n",
    "\n",
    "- x (Wagenposition) ∈ [-4.8, 4.8]\n",
    "- x’ (Wagengeschwindigkeit) ∈ [-3.4 10^38, 3.4 10^38]\n",
    "- theta (Neigungswinkel) ∈ [-0.42, 0.42]\n",
    "- theta’ (Winkelgeschwindigkeit) ∈ [-3.4 10^38, 3.4 10^38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def discretize(env, buckets, obs):\n",
    "    upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "    lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "    ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "    new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "    new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "    return tuple(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.statistics import plot\n",
    "\n",
    "n_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "def interact_with_environment(env, agent, buckets, verbose=False):\n",
    "    statistics = []\n",
    "    \n",
    "    if verbose:\n",
    "        print('Startposition:')\n",
    "        env.render()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        state = discretize(env, buckets, state) # transform state\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = discretize(env, buckets, next_state)\n",
    "            \n",
    "            agent.train((state, action, next_state, reward, done))\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'episode: {episode}/{n_episodes}, score: {total_reward}, steps:{t}, e:{agent.epsilon:.2f}')\n",
    "            env.render()\n",
    "            \n",
    "        statistics.append({\n",
    "            'episode': episode,\n",
    "            'score': total_reward,\n",
    "            'steps': t\n",
    "        })\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "Laut Schätzungen von Astonomen besteht das Universum aus etwa $10^{80}$ Atome. Ausgeschrieben sind das  100000000000000000000000000000000000000000000000000000000000000000000000000000000 Atome. Wie viele Zustände können in dem CartPole Environment auftreten, wenn die oben angegebenen Wertebereiche betrachtet werden? Welches Problem ergibt sich daraus für die Reinforcement Learning Aufgabe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Implementiere in **agent.py** einen Agenten, der in der Lage ist das CartPole Environment zu lösen. Definiere dafür eine angemessene Diskretisierung der state-spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_solution import AdvancedQLearning\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparams\n",
    "gamma = 0.75\n",
    "epsilon = 0.001\n",
    "epsilon_min = 0.0001\n",
    "alpha = 1.0\n",
    "alpha_min = 0.001\n",
    "buckets = (1, 1, 1, 1) # TODO: Define appropriate bucket sizes\n",
    "\n",
    "agent = AdvancedQLearning(action_size=action_size, buckets=buckets, gamma=gamma, \n",
    "                          epsilon=epsilon, epsilon_min=epsilon_min, \n",
    "                          alpha=alpha, alpha_min=alpha_min)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, buckets, verbose=False)\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "Benutze GridSearch, um die Hyperparameter zu optimieren. Der `score` sollte bei `200` konvergieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hyperparameter_optimization import GridSearch\n",
    "from functools import partial\n",
    "\n",
    "grid_search = GridSearch(\n",
    "    grid_params = {\n",
    "        'gamma': [], # TODO\n",
    "        'epsilon': [], # TODO\n",
    "        'alpha': [], # TODO\n",
    "        'alpha_min': [], # TODO\n",
    "        'epsilon_min': [], # TODO\n",
    "        'buckets': [] # TODO\n",
    "    },\n",
    "    fixed_params = {\n",
    "        'action_size': env.action_space.n\n",
    "    },\n",
    "    construct_env = partial(gym.make, 'CartPole-v0'),\n",
    "    construct_agent = AdvancedQLearning,\n",
    "    evaluation_func = interact_with_environment,\n",
    "    grid_params_for_evaluation_func = ['buckets'],\n",
    "    score_parameter = 'score'\n",
    ")\n",
    "grid_search.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
